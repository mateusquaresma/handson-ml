{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hams = [\"20021010_easy_ham.tar.bz2\", \"20021010_hard_ham.tar.bz2\", \n",
    "        \"20030228_easy_ham.tar.bz2\", \"20030228_easy_ham_2.tar.bz2\",\n",
    "        \"20030228_hard_ham.tar.bz2\"]\n",
    "spams = [\"20021010_spam.tar.bz2\",\"20030228_spam.tar.bz2\",\n",
    "         \"20030228_spam_2.tar.bz2\",\"20050311_spam_2.tar.bz2\"]\n",
    "\n",
    "def fetch_ham_data(ham_url=SPAM_URL, ham_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_spam_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "ham_filenames = [name for name in os.listdir(HAM_DIR) if len(name) > 20]\n",
    "spam_filenames = [name for name in os.listdir(SPAM_DIR) if len(name) > 20]\n",
    "print(len(ham_filenames))\n",
    "print(len(spam_filenames))\n",
    "ham_filenames = ham_filenames\n",
    "spam_filenames = spam_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy as policy\n",
    "from email.parser import BytesParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montando duas listas de e-mails, uma para cada fonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(filenames, directory):\n",
    "    emails = []\n",
    "    for fname in filenames:\n",
    "        with open(directory + \"/\" + fname, mode=\"rb\") as file:\n",
    "            emails.append(BytesParser(policy=policy.default).parse(file))\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = load_emails(ham_filenames, HAM_DIR)\n",
    "spam_emails = load_emails(spam_filenames, SPAM_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posso encapsular a estração das características em uma classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def content(self, email):\n",
    "        words = []\n",
    "        if type(email).__name__ == \"EmailMessage\":\n",
    "            if email.is_multipart():\n",
    "                words = words + self.content([e for e in email.get_payload()])\n",
    "            else:\n",
    "                try:\n",
    "                    if type(email.get_content()).__name__ == \"str\":\n",
    "                        new_words_raw = self.splitter.split(email.get_content())\n",
    "                        # print(new_words_raw)\n",
    "                        new_words = []\n",
    "                        for word in new_words_raw:\n",
    "                            if self.convert_to_lowercase:\n",
    "                                word = word.lower()\n",
    "                            if self.remove_punctuation:\n",
    "                                word = punctuation_re.sub(\"\", word)\n",
    "                            if self.replace_numbers:\n",
    "                                word = numbers_re.sub(\"NUMBER\", word)\n",
    "                            if self.replace_urls:\n",
    "                                word = urls_re.sub(\"URL\", word)\n",
    "\n",
    "                            new_words.append(word)    \n",
    "\n",
    "                        words += new_words\n",
    "                except:\n",
    "                    print('')\n",
    "\n",
    "        return words\n",
    "    \n",
    "    def fetch_words(self, emails=[]):\n",
    "        content_ = []\n",
    "        for e in emails:\n",
    "            content_ += self.content(e)\n",
    "        return content_\n",
    "    \n",
    "    def feature_vector(self, email, word_dict):\n",
    "        from collections import OrderedDict\n",
    "        occurrences = OrderedDict()\n",
    "        email_content = self.content(email)\n",
    "        for word in word_dict:\n",
    "            occurrences[word] = 1 if word in email_content else 0\n",
    "        return list(occurrences.values())    \n",
    "    \n",
    "    def build_features(self, emails=[]):\n",
    "        data = []\n",
    "        for email in emails:\n",
    "            data.append(self.feature_vector(email, self.vocabulary))\n",
    "        return data\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return self.vocabulary\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocabulary=[],\n",
    "                 remove_punctuation=False, \n",
    "                 replace_numbers=False, \n",
    "                 replace_urls=False, \n",
    "                 convert_to_lowercase=False):\n",
    "        \n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.replace_urls = replace_urls\n",
    "        self.convert_to_lowercase = convert_to_lowercase\n",
    "        self.vocabulary = vocabulary\n",
    "        \n",
    "        patterns = [\"\\\"\", \"\\'\", \"\\.\", \"!\", \"\\?\", \":\", \";\", \",\", \"\\(\", \"\\)\", \n",
    "                    \"\\*\", \"\\#\", \"[-]{2,30}\", \"\\|\", \"\\[\", \"\\]\", \"/\", \">\", \"<\", \"[_]{2,100}\"]\n",
    "        self.punctuation_re = re.compile(\"|\".join(patterns))\n",
    "        self.numbers_re = re.compile(\"\\d+\")\n",
    "        self.urls_re = re.compile(\"(www|http|https)+[^\\s]+[\\w]\")\n",
    "        self.splitter = re.compile(\"\\s|\\n|,\")\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        from collections import Counter\n",
    "        if len(self.vocabulary) == 0:\n",
    "            words = self.fetch_words(X)\n",
    "            word_counter = Counter(words)\n",
    "            minimum_accepted = int(len(X) * 0.1)\n",
    "            selected_items = [(word, count) for (word, count) in word_counter.items() if count > minimum_accepted]\n",
    "            self.vocabulary = [word for (word, _) in selected_items]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.build_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(remove_punctuation=True, \n",
    "                             replace_numbers=True, \n",
    "                             replace_urls=True, \n",
    "                             convert_to_lowercase=True)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"extractor\", extractor)\n",
    "])\n",
    "\n",
    "ham_features = pipeline.fit_transform(ham_emails)\n",
    "spam_features = pipeline.fit_transform(spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_df = pd.DataFrame(ham_features, columns=extractor.get_vocabulary())\n",
    "spam_df = pd.DataFrame(spam_features, columns=extractor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_df['label'] = 0\n",
    "spam_df['label'] = 1\n",
    "df = pd.concat([ham_df, spam_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando os conjuntos de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=32)\n",
    "train_set, valid_set = train_test_split(df, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set[\"label\"]\n",
    "X_train = train_set.drop(\"label\", axis=1)\n",
    "\n",
    "y_valid = valid_set[\"label\"]\n",
    "X_valid = valid_set.drop(\"label\", axis=1)\n",
    "\n",
    "y_test = test_set[\"label\"]\n",
    "X_test = test_set.drop(\"label\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando alguns classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = SVC(gamma=\"scale\")\n",
    "lin_scores = cross_val_score(lin_clf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3)\n",
    "lin_y_pred = cross_val_predict(lin_clf, X_train, y_train, cv=3)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_scores = cross_val_score(tree_clf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3)\n",
    "tree_y_pred = cross_val_predict(tree_clf, X_train, y_train, cv=3)\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier()\n",
    "sgd_scores = cross_val_score(sgd_clf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3)\n",
    "sgd_y_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3)\n",
    "sgd_rmse_scores = np.sqrt(-sgd_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=10)\n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=3)\n",
    "forest_y_pred = cross_val_predict(forest_clf, X_train, y_train, cv=3)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Standard deviation \", scores.std())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [0.2236068  0.18371173 0.21505813]\n",
      "Mean:  0.20745888671159432\n",
      "Standard deviation  0.017150616832032655\n",
      "\n",
      "Scores:  [0.31622777 0.29154759 0.30618622]\n",
      "Mean:  0.30465385953566676\n",
      "Standard deviation  0.010133732613845032\n",
      "\n",
      "Scores:  [0.25248762 0.25980762 0.22912878]\n",
      "Mean:  0.24714134311405853\n",
      "Standard deviation  0.013082680297981849\n",
      "\n",
      "Scores:  [0.27386128 0.25248762 0.25248762]\n",
      "Mean:  0.25961217522356234\n",
      "Standard deviation  0.010075637731199715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_scores(lin_rmse_scores)\n",
    "display_scores(tree_rmse_scores)\n",
    "display_scores(sgd_rmse_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "def display_m_scores(clf, y, y_pred):\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    rec = recall_score(y, y_pred)\n",
    "    pre = precision_score(y, y_pred)\n",
    "    f1  = f1_score(y, y_pred)\n",
    "    print(\"%s [accuracy=%.2f, recall=%.2f, precision=%.2f, f1=%.2f]\" % (clf, acc, rec, pre, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc [accuracy=0.96, recall=0.93, precision=0.84, f1=0.88]\n",
      "tree [accuracy=0.91, recall=0.86, precision=0.69, f1=0.77]\n",
      "sgd [accuracy=0.94, recall=0.94, precision=0.77, f1=0.85]\n",
      "forest [accuracy=0.93, recall=0.80, precision=0.80, f1=0.80]\n"
     ]
    }
   ],
   "source": [
    "display_m_scores(\"svc\", y_train, lin_y_pred)\n",
    "display_m_scores(\"tree\", y_train, tree_y_pred)\n",
    "display_m_scores(\"sgd\", y_train, sgd_y_pred)\n",
    "display_m_scores(\"forest\", y_train, forest_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procurando hyperparametros melhores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search(search_obj, attributes):\n",
    "    #cvres = search_obj.cv_results_\n",
    "    print(search_obj.best_estimator_)\n",
    "#     feature_importances = search_obj.best_estimator_.feature_importances_\n",
    "#     for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "#         print(np.sqrt(-mean_score), params)\n",
    "#     feat_with_relevance = sorted(zip(feature_importances, attributes), reverse=True)\n",
    "#     print(feat_with_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "lin_param_dist = {\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"class_weight\": [\"balanced\", None]\n",
    "}\n",
    "lin_rnd_search = RandomizedSearchCV(lin_clf, param_distributions=lin_param_dist, n_iter=5, cv=3)\n",
    "lin_rnd_search.fit(X_train, y_train)  \n",
    "evaluate_search(lin_rnd_search, extractor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "tree_param_dist = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"class_weight\": [\"balanced\", None]\n",
    "}\n",
    "tree_rnd_search = RandomizedSearchCV(tree_clf, param_distributions=tree_param_dist, n_iter=4, cv=3)\n",
    "tree_rnd_search.fit(X_train, y_train)\n",
    "evaluate_search(tree_rnd_search, extractor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.01, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='adaptive', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "sgd_param_dist = {\n",
    "    \"learning_rate\": [\"optimal\", \"adaptive\", \"invscaling\"],\n",
    "    \"class_weight\": [\"balanced\", None],\n",
    "    \"eta0\": [0.1, 0.01, 0.001]\n",
    "}\n",
    "sgd_rnd_search = RandomizedSearchCV(sgd_clf, param_distributions=sgd_param_dist, n_iter=5, cv=3)\n",
    "sgd_rnd_search.fit(X_train, y_train)\n",
    "evaluate_search(sgd_rnd_search, extractor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "forest_param_dist = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"class_weight\": [\"balanced\", None] \n",
    "}\n",
    "forest_rnd_search = RandomizedSearchCV(forest_clf, param_distributions=forest_param_dist, n_iter=5, cv=3)\n",
    "forest_rnd_search.fit(X_train, y_train)\n",
    "evaluate_search(forest_rnd_search, extractor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
